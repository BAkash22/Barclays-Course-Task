{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092a83e7-36c6-4dae-b900-751cdfd88005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990efccf-84fd-4103-b216-9cfc28fdae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0784ec8-aac6-43f1-9ae3-87d29d5e28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f065337a-6b6c-48fb-b046-dda2c6378f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93efd34e-22ba-4061-9cc8-dcdf261a42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e419bd-be9f-4b1e-944d-463905c7457e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.9.142.57:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a905826790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988becfb-f304-458d-b489-2d070d794ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.csv('customer_demographic_modified.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d472ba62-92ad-4f53-a178-f24d239dfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1aff7621-2194-4919-864c-181b6085772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "|      CSID|Income|dependants|Gender|Birth date|Relationship Start date|Marital_Status|Zip_Code|\n",
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "|2345450601|200000|         3|     ?|03/08/1976|               8/6/2017|      Divorced|CH42 0HS|\n",
      "|2345450602|100000|         2|     ?|28/11/1963|               8/3/1983|      Divorced|ME20 6PR|\n",
      "|2345450603|280000|         2|     ?|05/06/1979|               1/9/2008|       Married|OX17 1EQ|\n",
      "|2345450604|300000|         1|     ?|09/05/1999|             10/12/2020|      Divorced| RH7 6LT|\n",
      "|2345450605|150000|         1|  Male|31/03/2004|               6/3/2011|       Married| NW2 7RJ|\n",
      "|2345450606|100000|         3|     ?|18/02/1987|             17-12-2002|        Single| TQ1 3PZ|\n",
      "|2345450607|130000|         2|     M|09/12/1961|             27-11-1983|      Divorced| LU6 3NP|\n",
      "|2345450608|130000|         1|Female|23/02/1978|             15-11-1994|       Married|DA14 6NY|\n",
      "|2345450609|235000|         3|     F|02/02/1967|             29-08-1996|        Single|RH16 2QL|\n",
      "|2345450610|300000|         3|Female|13/03/1988|             25-04-2011|       Married|PO37 6EG|\n",
      "|2345450611|200000|         2|     ?|10/09/1975|               1/8/1997|      Divorced|BB11 4HW|\n",
      "|2345450612|250000|         2|Female|12/08/1992|             18-01-2000|        Single| TN4 9JU|\n",
      "|2345450613|200000|         0|Female|17/09/1988|             26-06-2017|       Widowed|SA67 8QB|\n",
      "|2345450614|500000|         2|  Male|11/09/1982|               3/5/2003|       Married| TA1 4XD|\n",
      "|2345450615|120000|         0|  Male|13/01/1991|             18-09-2022|        Single|WF12 8AZ|\n",
      "|2345450616|150000|         1|     ?|24/12/1963|             30-12-1987|       Married| WA8 9UF|\n",
      "|2345450617|150000|         0|Female|10/02/1989|             14-12-2011|        Single|  B2 4DU|\n",
      "|2345450618|300000|         0|     ?|11/11/1992|             28-02-2013|       Married| LA4 6DA|\n",
      "|2345450619|150000|         2|     ?|27/10/1992|             18-08-2004|       Married| SN3 6DR|\n",
      "|2345450620|250000|         3|     ?|27/10/1992|             26-03-1994|       Married|ME15 8BW|\n",
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DateConversion\").getOrCreate()\n",
    "\n",
    "df1= spark.read.csv('customer_demographic_modified.csv', header=True, inferSchema=True)\n",
    "\n",
    "def convert_date_format(date_str):\n",
    "\n",
    "    date_formats = [\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y/%B/%d\",\n",
    "        \"%Y/%b/%d\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d/%m/%Y\"\n",
    "    ]\n",
    "\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            return date_obj.strftime(\"%d/%m/%Y\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return date_str\n",
    "\n",
    "convert_date_format_udf = udf(convert_date_format, StringType())\n",
    "\n",
    "df1_cleaned = df1.withColumn(\"Birth date\", convert_date_format_udf(col(\"Birth date\")))\n",
    "\n",
    "df1_cleaned.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8a629a6-6326-4174-9659-ddae69446071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|      CSID|Income|dependants|       Gender|Birth date|Relationship Start date|Marital_Status|Zip_Code|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|2345450601|200000|         3|Not specified|03/08/1976|             08/06/2017|      Divorced|CH42 0HS|\n",
      "|2345450602|100000|         2|Not specified|28/11/1963|             08/03/1983|      Divorced|ME20 6PR|\n",
      "|2345450603|280000|         2|Not specified|05/06/1979|             01/09/2008|       Married|OX17 1EQ|\n",
      "|2345450604|300000|         1|Not specified|09/05/1999|             10/12/2020|      Divorced| RH7 6LT|\n",
      "|2345450605|150000|         1|         Male|31/03/2004|             06/03/2011|       Married| NW2 7RJ|\n",
      "|2345450606|100000|         3|Not specified|18/02/1987|             17/12/2002|        Single| TQ1 3PZ|\n",
      "|2345450607|130000|         2|         Male|09/12/1961|             27/11/1983|      Divorced| LU6 3NP|\n",
      "|2345450608|130000|         1|       Female|23/02/1978|             15/11/1994|       Married|DA14 6NY|\n",
      "|2345450609|235000|         3|       Female|02/02/1967|             29/08/1996|        Single|RH16 2QL|\n",
      "|2345450610|300000|         3|       Female|13/03/1988|             25/04/2011|       Married|PO37 6EG|\n",
      "|2345450611|200000|         2|Not specified|10/09/1975|             01/08/1997|      Divorced|BB11 4HW|\n",
      "|2345450612|250000|         2|       Female|12/08/1992|             18/01/2000|        Single| TN4 9JU|\n",
      "|2345450613|200000|         0|       Female|17/09/1988|             26/06/2017|       Widowed|SA67 8QB|\n",
      "|2345450614|500000|         2|         Male|11/09/1982|             03/05/2003|       Married| TA1 4XD|\n",
      "|2345450615|120000|         0|         Male|13/01/1991|             18/09/2022|        Single|WF12 8AZ|\n",
      "|2345450616|150000|         1|Not specified|24/12/1963|             30/12/1987|       Married| WA8 9UF|\n",
      "|2345450617|150000|         0|       Female|10/02/1989|             14/12/2011|        Single|  B2 4DU|\n",
      "|2345450618|300000|         0|Not specified|11/11/1992|             28/02/2013|       Married| LA4 6DA|\n",
      "|2345450619|150000|         2|Not specified|27/10/1992|             18/08/2004|       Married| SN3 6DR|\n",
      "|2345450620|250000|         3|Not specified|27/10/1992|             26/03/1994|       Married|ME15 8BW|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def standardize_gender(gender):\n",
    "    gender_map = {\n",
    "        \"F\": \"Female\",\n",
    "        \"M\": \"Male\",\n",
    "        \"?\": \"Not specified\",\n",
    "        \"\": \"Not specified\",\n",
    "        \"Male\": \"Male\",\n",
    "        \"Female\": \"Female\"\n",
    "    }\n",
    "    return gender_map.get(gender, gender)\n",
    "\n",
    "standardize_gender_udf = udf(standardize_gender, StringType())\n",
    "\n",
    "df1_cleaned = df1_cleaned.withColumn(\"Gender\", standardize_gender_udf(col(\"Gender\")))\n",
    "\n",
    "\n",
    "df1_cleaned.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a481a80-0dc0-46a6-b4a2-feb37ea4bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|      CSID|Income|dependants|       Gender|Birth date|Relationship Start date|Marital_Status|Zip_Code|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|2345450601|200000|         3|Not specified|03/08/1976|             08/06/2017|      Divorced|CH42 0HS|\n",
      "|2345450602|100000|         2|Not specified|28/11/1963|             08/03/1983|      Divorced|ME20 6PR|\n",
      "|2345450603|280000|         2|Not specified|05/06/1979|             01/09/2008|       Married|OX17 1EQ|\n",
      "|2345450604|300000|         1|Not specified|09/05/1999|             10/12/2020|      Divorced| RH7 6LT|\n",
      "|2345450605|150000|         1|         Male|31/03/2004|             06/03/2011|       Married| NW2 7RJ|\n",
      "|2345450606|100000|         3|Not specified|18/02/1987|             17/12/2002|        Single| TQ1 3PZ|\n",
      "|2345450607|130000|         2|         Male|09/12/1961|             27/11/1983|      Divorced| LU6 3NP|\n",
      "|2345450608|130000|         1|       Female|23/02/1978|             15/11/1994|       Married|DA14 6NY|\n",
      "|2345450609|235000|         3|       Female|02/02/1967|             29/08/1996|        Single|RH16 2QL|\n",
      "|2345450610|300000|         3|       Female|13/03/1988|             25/04/2011|       Married|PO37 6EG|\n",
      "|2345450611|200000|         2|Not specified|10/09/1975|             01/08/1997|      Divorced|BB11 4HW|\n",
      "|2345450612|250000|         2|       Female|12/08/1992|             18/01/2000|        Single| TN4 9JU|\n",
      "|2345450613|200000|         0|       Female|17/09/1988|             26/06/2017|       Widowed|SA67 8QB|\n",
      "|2345450614|500000|         2|         Male|11/09/1982|             03/05/2003|       Married| TA1 4XD|\n",
      "|2345450615|120000|         0|         Male|13/01/1991|             18/09/2022|        Single|WF12 8AZ|\n",
      "|2345450616|150000|         1|Not specified|24/12/1963|             30/12/1987|       Married| WA8 9UF|\n",
      "|2345450617|150000|         0|       Female|10/02/1989|             14/12/2011|        Single|  B2 4DU|\n",
      "|2345450618|300000|         0|Not specified|11/11/1992|             28/02/2013|       Married| LA4 6DA|\n",
      "|2345450619|150000|         2|Not specified|27/10/1992|             18/08/2004|       Married| SN3 6DR|\n",
      "|2345450620|250000|         3|Not specified|27/10/1992|             26/03/1994|       Married|ME15 8BW|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_date_format(date_str):\n",
    "\n",
    "    date_formats = [\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y/%B/%d\",\n",
    "        \"%Y/%b/%d\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d/%m/%Y\"\n",
    "    ]\n",
    "\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            return date_obj.strftime(\"%d/%m/%Y\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return date_str\n",
    "\n",
    "convert_date_format_udf = udf(convert_date_format, StringType())\n",
    "\n",
    "df1_cleaned = df1_cleaned.withColumn(\"Relationship Start date\", convert_date_format_udf(col(\"Relationship Start date\")))\n",
    "\n",
    "df1_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b011554-287e-4138-ab8a-20076675ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 2 -df2 - customer channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3bf825c2-3cc4-419d-8855-4b560b694517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.read.csv('Customer_Channel_Activity_modified.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e394d6d7-b414-48d1-9f73-32585d6352aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------+\n",
      "|      CSID|  Channel|ActivityDate|   ActivityType|\n",
      "+----------+---------+------------+---------------+\n",
      "|2345450601|Telephone|  2022-05-28|Account Opening|\n",
      "|2345450611|      App|  1987-05-21|Account Opening|\n",
      "|2345450616|   Branch|  01/05/2024|Account Opening|\n",
      "|2345450635|      App|  17/01/2022|Account Opening|\n",
      "|2345450637|      App|  18/11/2023|Account Opening|\n",
      "+----------+---------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f6c700e-9534-41ee-bdd1-68ab656bf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|      CSID|Income|dependants|       Gender|Birth date|Relationship Start date|Marital_Status|Zip_Code|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "|2345450601|200000|         3|Not specified|03/08/1976|             08/06/2017|      Divorced|CH42 0HS|\n",
      "|2345450602|100000|         2|Not specified|28/11/1963|             08/03/1983|      Divorced|ME20 6PR|\n",
      "|2345450603|280000|         2|Not specified|05/06/1979|             01/09/2008|       Married|OX17 1EQ|\n",
      "|2345450604|300000|         1|Not specified|09/05/1999|             10/12/2020|      Divorced| RH7 6LT|\n",
      "|2345450605|150000|         1|         Male|31/03/2004|             06/03/2011|       Married| NW2 7RJ|\n",
      "+----------+------+----------+-------------+----------+-----------------------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e10f5e1-2224-4c8c-be80-66dd4ba36a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (2.0.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akash\\anaconda3\\envs\\pyspark\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/250.9 kB 393.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 215.0/250.9 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.9/250.9 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c0172e9-12e6-41c0-a06f-b70f4c81a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_cleaned_pandas = df1_cleaned.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9be6ee94-8595-4d9c-ba0c-7688b176dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_cleaned_pandas.to_csv('C:/Users/akash/Documents/Final task/cleaned_dataset/customer_demographic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d1ed9ace-466e-416e-82f5-1b38480e5d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------+\n",
      "|      CSID|  Channel|ActivityDate|   ActivityType|\n",
      "+----------+---------+------------+---------------+\n",
      "|2345450601|Telephone|  28/05/2022|Account Opening|\n",
      "|2345450611|      App|  21/05/1987|Account Opening|\n",
      "|2345450616|   Branch|  01/05/2024|Account Opening|\n",
      "|2345450635|      App|  17/01/2022|Account Opening|\n",
      "|2345450637|      App|  18/11/2023|Account Opening|\n",
      "|2345450640|Telephone|  19/08/2009|Account Opening|\n",
      "|2345450641|   Online|  05/01/2017|Account Opening|\n",
      "|2345450643|Telephone|  10/04/2024|Account Opening|\n",
      "|2345450645|   Online|  01/04/2009|Account Opening|\n",
      "|2345450647|Telephone|  05/05/2006|Account Opening|\n",
      "|2345450652|   Branch|  17/08/2007|Account Opening|\n",
      "|2345450657|      App|  20/03/2020|Account Opening|\n",
      "|2345450658|Telephone|  19/02/2010|Account Opening|\n",
      "|2345450664|   Online|  07/01/2014|Account Opening|\n",
      "|2345450667|Telephone|  11/10/2016|Account Opening|\n",
      "|2345450671|   Branch|  29/07/2017|Account Opening|\n",
      "|2345450672|   Online|  15/08/2023|Account Opening|\n",
      "|2345450684|   Branch|  04/05/2019|Account Opening|\n",
      "|2345450698|   Branch|  09/12/2013|Account Opening|\n",
      "|2345450699|   Online|  24/03/2018|Account Opening|\n",
      "+----------+---------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DateConversion\").getOrCreate()\n",
    "\n",
    "df= spark.read.csv('Customer_Channel_Activity_modified.csv', header=True, inferSchema=True)\n",
    "\n",
    "def convert_date_format(date_str):\n",
    "    date_formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%m/%d/%Y\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%m-%d-%Y\",\n",
    "        \"%d %B %Y\"\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            return date_obj.strftime(\"%d/%m/%Y\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "convert_date_format_udf = udf(convert_date_format, StringType())\n",
    "\n",
    "df_cleaned = df.withColumn(\"ActivityDate\", convert_date_format_udf(col(\"ActivityDate\")))\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637391f7-f815-40d3-951b-6a39f65c16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_pandas.to_csv('C:/Users/akash/Documents/Final task/cleaned_dataset/Customer_Channel_Activity_modified.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
